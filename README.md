# ğŸ¤– Agent Knowledge Interne

Un assistant IA interne capable de lire les documents d'entreprise et de rÃ©pondre aux questions des employÃ©s en langage naturel.

## ğŸ¯ FonctionnalitÃ©s

- **ğŸ“š Indexation automatique** des documents (PDF, Word, Markdown, TXT)
- **ğŸ” Recherche sÃ©mantique** avec ChromaDB
- **ğŸ’¬ Interface de chat** intuitive avec Streamlit
- **ğŸ¤– RÃ©ponses contextuelles** avec Ollama (modÃ¨le local)
- **ğŸ“Š Sources citÃ©es** pour chaque rÃ©ponse
- **ğŸ”’ SÃ©curitÃ©** : tout fonctionne en local
- **ğŸ³ DÃ©ploiement Docker** simple

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

- Python 3.11+
- Docker et Docker Compose
- 8GB RAM minimum (pour le modÃ¨le Llama)

### Installation

1. **Cloner le projet**
```bash
git clone <repository-url>
cd agentia
```

2. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

3. **Configurer l'environnement**
```bash
cp config.env.example config.env
# Ã‰diter config.env selon vos besoins
```

4. **DÃ©marrer avec Docker Compose**
```bash
docker-compose up --build
```

5. **Ou dÃ©marrer manuellement**
```bash
# Terminal 1: DÃ©marrer Ollama
docker run -d -p 11434:11434 ollama/ollama:latest

# Terminal 2: TÃ©lÃ©charger le modÃ¨le
docker exec <container-id> ollama pull llama3:8b

# Terminal 3: DÃ©marrer l'application
python start.py
```

6. **AccÃ©der Ã  l'application**
Ouvrez votre navigateur sur : http://localhost:8501

## ğŸ“ Structure du Projet

```
agentia/
â”œâ”€â”€ src/                    # Code source principal
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”œâ”€â”€ document_processor.py # Traitement des documents
â”‚   â”œâ”€â”€ vector_store.py    # Base vectorielle ChromaDB
â”‚   â””â”€â”€ llm_client.py      # Client Ollama
â”œâ”€â”€ data/                  # Documents Ã  indexer
â”œâ”€â”€ chroma_db/            # Base de donnÃ©es vectorielle
â”œâ”€â”€ logs/                 # Fichiers de logs
â”œâ”€â”€ app.py                # Interface Streamlit
â”œâ”€â”€ start.py              # Script de dÃ©marrage
â”œâ”€â”€ test_agent.py         # Tests
â”œâ”€â”€ docker-compose.yml    # Configuration Docker
â”œâ”€â”€ Dockerfile           # Image Docker
â””â”€â”€ requirements.txt     # DÃ©pendances Python
```

## ğŸ”§ Configuration

### Variables d'environnement (config.env)

```env
# ModÃ¨le LLM
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3:8b

# Base vectorielle
CHROMA_PERSIST_DIRECTORY=./chroma_db
CHROMA_COLLECTION_NAME=enterprise_documents

# ModÃ¨le d'embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Limites
MAX_DOCUMENT_SIZE_MB=50
MAX_CHUNK_SIZE=1000
MAX_CHUNKS_PER_DOCUMENT=100
```

## ğŸ“š Utilisation

### 1. Ajouter des Documents

Placez vos documents dans le dossier `data/` :
- **PDF** : `.pdf`
- **Word** : `.docx`, `.doc`
- **Markdown** : `.md`
- **Texte** : `.txt`

### 2. Indexer les Documents

Dans l'interface web :
1. Allez dans la sidebar
2. Cliquez sur "Indexer tous les documents du dossier"
3. Attendez la fin du traitement

### 3. Poser des Questions

Exemples de questions :
- "Quelle est la politique de tÃ©lÃ©travail ?"
- "Comment rÃ©initialiser le serveur de staging ?"
- "Quelle est la procÃ©dure de remboursement ?"

## ğŸ§ª Tests

ExÃ©cuter les tests :
```bash
python test_agent.py
```

Les tests vÃ©rifient :
- âœ… Traitement des documents
- âœ… Base vectorielle ChromaDB
- âœ… Connexion Ollama
- âœ… Agent complet

## ğŸ”’ SÃ©curitÃ©

- **DonnÃ©es locales** : Aucune donnÃ©e n'est envoyÃ©e Ã  l'extÃ©rieur
- **ModÃ¨le local** : Ollama fonctionne entiÃ¨rement en local
- **Chiffrement** : ChromaDB stocke les donnÃ©es localement
- **Authentification** : PrÃªt pour intÃ©gration OAuth (Ã  implÃ©menter)

## ğŸš€ DÃ©ploiement Production

### Avec Docker Compose

```bash
# Production
docker-compose -f docker-compose.prod.yml up -d
```

### Serveur DÃ©diÃ©

1. **Serveur Ubuntu 20.04+**
2. **Installation Docker**
3. **Configuration reverse proxy** (Nginx)
4. **SSL/TLS** avec Let's Encrypt
5. **Monitoring** avec Prometheus/Grafana

## ğŸ“Š Monitoring

### MÃ©triques Disponibles

- Nombre de documents indexÃ©s
- Temps de rÃ©ponse moyen
- Score de confiance des rÃ©ponses
- Utilisation des ressources

### Logs

Les logs sont disponibles dans :
- `logs/agent.log` : Logs de l'application
- Docker logs : `docker-compose logs -f`

## ğŸ”§ DÃ©veloppement

### Structure du Code

- **`config.py`** : Configuration centralisÃ©e
- **`document_processor.py`** : Extraction et traitement des documents
- **`vector_store.py`** : Gestion ChromaDB et embeddings
- **`llm_client.py`** : Interface Ollama et agent principal
- **`app.py`** : Interface utilisateur Streamlit

### Ajouter de Nouveaux Formats

1. Ã‰tendre `DocumentProcessor` dans `document_processor.py`
2. Ajouter la mÃ©thode d'extraction correspondante
3. Mettre Ã  jour `supported_extensions`

### IntÃ©grations Futures

- **Google Drive API** : Synchronisation automatique
- **Notion API** : Import des pages internes
- **Slack Bot** : Interface dans Slack
- **SharePoint** : Connexion aux documents SharePoint

## ğŸ› DÃ©pannage

### Ollama ne dÃ©marre pas

```bash
# VÃ©rifier Docker
docker ps

# RedÃ©marrer Ollama
docker restart ollama-agentia

# VÃ©rifier les logs
docker logs ollama-agentia
```

### Erreur de mÃ©moire

- RÃ©duire `MAX_CHUNK_SIZE` dans `config.env`
- Utiliser un modÃ¨le plus petit (llama3:7b au lieu de 8b)
- Augmenter la RAM du serveur

### Documents non indexÃ©s

- VÃ©rifier les formats supportÃ©s
- ContrÃ´ler la taille des fichiers (< 50MB)
- Consulter les logs dans `logs/agent.log`

## ğŸ“ˆ Roadmap

### Version 1.1
- [ ] Authentification OAuth
- [ ] Interface d'administration
- [ ] API REST
- [ ] IntÃ©gration Slack

### Version 1.2
- [ ] Support multimÃ©dia (images, diagrammes)
- [ ] Workflow automatisÃ©
- [ ] Analytics avancÃ©es
- [ ] Mode hors ligne

### Version 2.0
- [ ] Agents multiples (CrewAI)
- [ ] GÃ©nÃ©ration automatique de FAQ
- [ ] IntÃ©gration CRM
- [ ] Mobile app

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit vos changements (`git commit -am 'Ajout nouvelle fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. CrÃ©er une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ“ Support

- **Issues** : Utilisez GitHub Issues
- **Email** : support@techcorp.com
- **Documentation** : [Wiki du projet](https://github.com/your-repo/wiki)

---

**DÃ©veloppÃ© avec â¤ï¸ pour amÃ©liorer l'efficacitÃ© des Ã©quipes internes**
